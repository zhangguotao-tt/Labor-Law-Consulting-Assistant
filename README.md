# 劳动法咨询助手
# 1、项目背景与需求设计：
该项目是一个基于RAG的劳动法法律条文智能问答助手。旨在帮助用户快速获得劳动法相关问题的解答。
## 1.1 法律依据文件：
本项目主要回答用户劳动法相关的问题，法律依据文件只包含：《中华人民共和国劳动法》、《中华人民共和国劳动合同法》，其他法律文件相关的问题，本项目不涉及。其他法律问题，可以基于此项目进行扩展。
## 1.2 核心需求：
法律条文智能问答系统，需满足：
1、用户输入问题，进行劳动法相关问题的提问，系统依据《中华人民共和国劳动法》、《中华人民共和国劳动合同法》，给出问题的回复。
2、系统进行回复时，需支持条款精准引用（如“《劳动法》第36条”）。
3、非以上法律相关问题，明确回复客户，该问题不在本系统回答范围内。
4、支持动态更新知识库。
## 1.3 业务流程：
用户提问 → 问题解析 → RAG检索 → 生成答案 → 引用溯源
# 2、技术选型
## 2.1 RAG
为了支持精准的法律依据引用，在回答用户时需利用已有的法律文件知识库，而不是由LLM直接回答客户问题，避免回答问题时出现“幻觉”的情况，增加回答问题的可靠性。因此，本系统采用RAG相关技术。
### 2.1.1 开发框架：
在实现RAG过程，主要采用llamaindex开发框架，进行RAG的构建。llamaindex框架包含：数据连接器、数据索引、引擎等多种工具，能够快速实现系统功能的开发。
### 2.1.2 重排序：
为了增加召回精度，通过两阶段检索机制，进行检索。
1、初筛阶段：使用向量检索获取候选集（Top10）。
2、精排阶段：通过专用重排序模型计算语义相关性。
## 2.2 向量数据库
本项目采用chroma向量数据库。
## 2.3 LLM选择
考虑到将系统应用到私有化企业数据等更常见的场景，本项目选择LLM时，主要基于本地模型部署。
如果是因为本地资源限制等原因（如无足够的显卡，不具备本地模型部署条件），也可切换使用在线模型。
因为本项目考虑了重排序，因此需要3个模型。
### 2.3.1 embedding
embedding模型，采用text2vec-base-chinese-sentence，从modlescope进行下载，部署到本地服务器。
### 2.3.2 LLM
1、受本地资源限制，模型选择较小的qwen1.5-1.8B，为保证更好的回答效果，在资源条件允许的情况下，尽量选择更大的模型。
2、使用本地qwen1.5-1.8B时，模型能力有限，针对法律条文的理解，进行了模型的微调。
3、系统本地部署采用vLLM。
3、程序中也测试使用了在线模型glm-4，在线模型参数量更大，效果更优。
### 2.3.3 重排序模型
重排序模型，采用bge-reranker-large，从modelscope进行下载，并部署到本地服务器。
## 2.4 前端页面
前端页面通过streamlit实现。streamlit能快速实现页面，进行系统功能的验证。
# 3、系统架构
1、系统整体架构：
<img width="506" height="562" alt="图片1" src="https://github.com/user-attachments/assets/9d2b70bd-9583-4767-9287-33f740266a2b" />

2、系统详细交互流程：
LLM向量库后端前端用户LLM向量库后端前端用户输入法律问题发送查询请求执行语义检索返回候选条款生成最终回答返回结构化结果展示专业解答查看支持依据。
<img width="731" height="415" alt="图片2" src="https://github.com/user-attachments/assets/b160500f-b68e-46e1-a0f7-58708c2dcb61" />

# 4、数据处理
要保证RAG良好的效果，对法律条文原始数据的处理，非常重要。原理的法律条文，需整理成格式统一的josn数据，以方便检索使用。
# 5、最终效果

<img width="1872" height="962" alt="image" src="https://github.com/user-attachments/assets/f92b87cb-7a5a-449b-8c55-7fcf92435b55" />

